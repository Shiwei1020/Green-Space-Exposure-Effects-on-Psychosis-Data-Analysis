{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1519b95-19e6-4a0b-8438-dbad2a9d2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b53275-8512-4635-a9ff-c0b111a45181",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_clinical = pd.read_csv('june22_clinical with ndvi norm.csv')\n",
    "filtered_daily = pd.read_csv('june22_daily ema with ndvi norm.csv')\n",
    "clinical_daily_final_merge = pd.read_csv('june22_merge clinical and daily.csv')\n",
    "\n",
    "# filtered_clinical = pd.read_csv('june22_clinical with ndvi norm_year.csv')\n",
    "# filtered_daily = pd.read_csv('june22_daily ema with ndvi norm_year.csv')\n",
    "# clinical_daily_final_merge = pd.read_csv('june22_merge clinical and daily_year.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3748c5b9-26c0-4a94-a0b2-9c3aa65d2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Step 1: Divide it into the individual levels (ndvi >=0.5) ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dde943-d8e5-468d-b25e-63455503cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### US CLINICAL #########\n",
    "us_clinical = filtered_clinical.copy()\n",
    "us_chr = us_clinical[us_clinical['Phenotype'] == 'CHR'].copy()\n",
    "us_hc = us_clinical[us_clinical['Phenotype'] == 'HC'].copy()\n",
    "\n",
    "us_high_all = filtered_clinical[filtered_clinical['ndvi_v4_mean'] >= 0.5].copy()\n",
    "us_low_all = filtered_clinical[filtered_clinical['ndvi_v4_mean'] < 0.5].copy()\n",
    "\n",
    "us_high_chr = us_high_all[us_high_all['Phenotype'] == 'CHR'].copy()\n",
    "us_low_chr = us_low_all[us_low_all['Phenotype'] == 'CHR'].copy()\n",
    "\n",
    "us_high_hc = us_high_all[us_high_all['Phenotype'] == 'HC'].copy()\n",
    "us_low_hc = us_low_all[us_low_all['Phenotype'] == 'HC'].copy()\n",
    "\n",
    "\n",
    "#### EMA + Clinical ####\n",
    "us_merged_daily_total =clinical_daily_final_merge.copy()\n",
    "us_merged_daily_chr = clinical_daily_final_merge[clinical_daily_final_merge['Phenotype'] == 'CHR'].copy()\n",
    "us_merged_daily_hc = clinical_daily_final_merge[clinical_daily_final_merge['Phenotype'] == 'HC'].copy()\n",
    "\n",
    "us_merged_daily_total_high = us_merged_daily_total[us_merged_daily_total['ndvi_v4'] >= 0.5].copy()\n",
    "us_merged_daily_chr_high = us_merged_daily_total_high[us_merged_daily_total_high['Phenotype'] == 'CHR'].copy()\n",
    "us_merged_daily_hc_high = us_merged_daily_total_high[us_merged_daily_total_high['Phenotype'] == 'HC'].copy()\n",
    "\n",
    "\n",
    "us_merged_daily_total_low = us_merged_daily_total[us_merged_daily_total['ndvi_v4'] < 0.5].copy()\n",
    "us_merged_daily_chr_low = us_merged_daily_total_low[us_merged_daily_total_low['Phenotype'] == 'CHR'].copy()\n",
    "us_merged_daily_hc_low = us_merged_daily_total_low[us_merged_daily_total_low['Phenotype'] == 'HC'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41e22f-f7bb-4e2a-bc5f-3ef860d7153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## US Maps Index\n",
    "\n",
    "#### Clinical ####\n",
    "# us_clinical\n",
    "# us_chr\n",
    "# us_hc\n",
    "\n",
    "# us_high_all\n",
    "# us_low_all\n",
    "\n",
    "# us_high_chr\n",
    "# us_low_chr\n",
    "\n",
    "# us_high_hc\n",
    "# us_low_hc\n",
    "\n",
    "#### EMA + Clinical ####\n",
    "# us_merged_daily_total\n",
    "# us_merged_daily_chr\n",
    "# us_merged_daily_hc\n",
    "\n",
    "# us_merged_daily_total_high\n",
    "# us_merged_daily_chr_high\n",
    "# us_merged_daily_hc_high\n",
    "\n",
    "# us_merged_daily_total_low\n",
    "# us_merged_daily_chr_low\n",
    "# us_merged_daily_hc_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3c053-50e1-40c9-a6a2-018a3ddd3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Step 2: Mediating Variables -------------\n",
    "\n",
    "### Varaibles:\n",
    "# 1. EMA passive phenotyping: Sleep duration, Home time, Screen time, Entropy\n",
    "# 2. Sociodemographic Information: Income, Education, Housing, Sex, Age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176651a4-99ba-4f24-801c-165d70fe7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import needed packages after code execution state reset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "setattr(pd, \"Float64Index\", pd.Index) # Hacky fix for ImportError.\n",
    "setattr(pd, \"Int64Index\", pd.Index)\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def run_mediation_moderation(\n",
    "    data,\n",
    "    mediate_var='percent_home_norm',\n",
    "    dv_var='control_norm',\n",
    "    iv_var='ndvi_v4',\n",
    "):\n",
    "    \n",
    "    \n",
    "    # ==== Step 1: Prepare data ====\n",
    "    df = data.copy()\n",
    "    # Z-score transformation\n",
    "    df[\"ndvi_z\"] = (df[iv_var] - df[iv_var].mean()) / df[iv_var].std()\n",
    "    df[\"mediating_z\"] = (df[mediate_var] - df[mediate_var].mean()) / df[mediate_var].std()\n",
    "\n",
    "    \n",
    "    # ==== Step 2: Fit Mediation Models ====\n",
    "    # a path: IV → Mediator\n",
    "    a_model = smf.ols(f\"{mediate_var} ~ {iv_var}\", data=df).fit()\n",
    "    print(\"A summary\")\n",
    "    print(a_model.summary())\n",
    "    print(\"-\" * 30)\n",
    "    a = a_model.params[iv_var]\n",
    "    a_p = a_model.pvalues[iv_var]\n",
    "\n",
    "    # b and c′ paths: Mediator + IV → DV\n",
    "    b_model = smf.ols(f\"{dv_var} ~ {iv_var} + {mediate_var}\", data=df).fit()\n",
    "    print(\"B summary\")\n",
    "    print(b_model.summary())\n",
    "    print(\"-\" * 30)\n",
    "    b = b_model.params[mediate_var]\n",
    "    b_p = b_model.pvalues[mediate_var]\n",
    "    c_prime = b_model.params[iv_var]\n",
    "    c_prime_p = b_model.pvalues[iv_var]\n",
    "\n",
    "    # Total effect c path: IV → DV\n",
    "    c_model = smf.ols(f\"{dv_var} ~ {iv_var}\", data=df).fit()\n",
    "    print(\"C summary\")\n",
    "    print(c_model.summary())\n",
    "    print(\"-\" * 30)\n",
    "    c = c_model.params[iv_var]\n",
    "    c_p = c_model.pvalues[iv_var]\n",
    "\n",
    "    # Indirect effect\n",
    "    indirect = a * b\n",
    "    \n",
    "     # Get standard errors from models\n",
    "    se_a = a_model.bse[iv_var]\n",
    "    se_b = b_model.bse[mediate_var]\n",
    "    # Sobel standard error\n",
    "    se_indirect = sqrt(b**2 * se_a**2 + a**2 * se_b**2)\n",
    "    # Sobel z-value\n",
    "    z_sobel = indirect / se_indirect\n",
    "    # Two-tailed p-value\n",
    "    indirect_p = 2 * (1 - norm.cdf(abs(z_sobel)))\n",
    "    # Print or use this in your summary\n",
    "    print(f\"Indirect Effect p-value (Sobel test): {indirect_p:.4f}\")\n",
    "\n",
    "\n",
    "    # ==== Step 3: Mediation Plot ====\n",
    "\n",
    "    def p_to_stars(p):\n",
    "        if p < 0.001: return \"***\"\n",
    "        elif p < 0.01: return \"**\"\n",
    "        elif p < 0.05: return \"*\"\n",
    "        else: return \"\"\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Nodes\n",
    "    ax.text(0.1, 0.5, iv_var, bbox=dict(facecolor=\"lightblue\", edgecolor=\"black\"), fontsize=12)\n",
    "    ax.text(0.5, 0.8, mediate_var, bbox=dict(facecolor=\"lightyellow\", edgecolor=\"black\"), fontsize=12)\n",
    "    ax.text(0.9, 0.5, dv_var, bbox=dict(facecolor=\"lightgreen\", edgecolor=\"black\"), fontsize=12)\n",
    "\n",
    "    # Arrows\n",
    "    def draw_arrow(xy_start, xy_end, text, fontsize=11):\n",
    "        ax.annotate(\"\", xy=xy_end, xytext=xy_start,\n",
    "                    arrowprops=dict(arrowstyle=\"->\", lw=2))\n",
    "        mid_x = (xy_start[0] + xy_end[0]) / 2\n",
    "        mid_y = (xy_start[1] + xy_end[1]) / 2\n",
    "        ax.text(mid_x, mid_y, text, fontsize=fontsize)\n",
    "\n",
    "    # Paths: c′, a, b\n",
    "    draw_arrow((0.18, 0.5), (0.82, 0.5), f\"{c_prime:.2f} {p_to_stars(c_prime_p)}\")  # direct\n",
    "    draw_arrow((0.18, 0.5), (0.5, 0.78), f\"{a:.2f} {p_to_stars(a_p)}\")              # a path\n",
    "    draw_arrow((0.5, 0.78), (0.82, 0.5), f\"{b:.2f} {p_to_stars(b_p)}\")              # b path\n",
    "\n",
    "    plt.title(\"Mediation Diagram\", fontsize=14)\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=== Mediation Summary ===\")\n",
    "    print(f\"a ({iv_var} → {mediate_var}): {a:.3f} {p_to_stars(a_p)}\")\n",
    "    print(f\"b ({mediate_var} → {dv_var}): {b:.3f} {p_to_stars(b_p)}\")\n",
    "    print(f\"Indirect Effect (a×b): {indirect:.3f} {p_to_stars(indirect_p)}\")\n",
    "    print(f\"Direct Effect (c′: {iv_var} → {dv_var} controlling for {mediate_var}): {c_prime:.3f} {p_to_stars(c_prime_p)}\")\n",
    "    print(f\"Total Effect (c: {iv_var} → {dv_var}): {c:.3f} {p_to_stars(c_p)}\")\n",
    "\n",
    "   \n",
    "    \n",
    "    # ==== Step 4: Moderation Plot ====\n",
    "\n",
    "    # Create interaction term\n",
    "    df[\"interaction\"] = df[iv_var] * df[mediate_var]\n",
    "    mod_model = smf.ols(f\"{dv_var} ~ {iv_var} * {mediate_var}\", data=df).fit()\n",
    "    interaction_p = mod_model.pvalues[f\"{iv_var}:{mediate_var}\"]\n",
    "    \n",
    "\n",
    "    print(mod_model.summary())\n",
    "    print(f\"Moderation Interaction Term p-value: {interaction_p:.3f} {p_to_stars(interaction_p)}\")\n",
    "    \n",
    "    # ==== Step 5: Summary Table ====\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Path': [\n",
    "            f'a path',               \n",
    "            f'b path',              \n",
    "            f'c′ path (direct)',           \n",
    "            f'c path (total)',            \n",
    "            f'Indirect Effect (a×b)',              \n",
    "            f'moderation'  \n",
    "        ],\n",
    "        'Coefficient': [\n",
    "            round(a, 3),\n",
    "            round(b, 3),\n",
    "            round(c_prime, 3),\n",
    "            round(c, 3),\n",
    "            round(indirect, 3),\n",
    "            round(mod_model.params.get(f\"{iv_var}:{mediate_var}\", np.nan), 3)\n",
    "        ],\n",
    "        'p-value': [\n",
    "            round(a_p, 3),\n",
    "            round(b_p, 3),\n",
    "            round(c_prime_p, 3),\n",
    "            round(c_p, 3),\n",
    "            round(indirect_p, 3),\n",
    "            round(interaction_p, 3)\n",
    "        ],\n",
    "        'Significance': [\n",
    "            p_to_stars(a_p),\n",
    "            p_to_stars(b_p),\n",
    "            p_to_stars(c_prime_p),\n",
    "            p_to_stars(c_p),\n",
    "            p_to_stars(indirect_p),\n",
    "            p_to_stars(interaction_p)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    print(\"\\n=== Summary Table ===\")\n",
    "    return summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e1894-450a-4c74-95e4-beebbde82c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Sample Data Analysis  ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eebfab-5893-4d1a-8fd6-97399cc19f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_mediation_moderation(\n",
    "#     data = us_merged_daily_chr,\n",
    "#     mediate_var='percent_home_norm',\n",
    "#     dv_var='bprs_total_m2_norm',\n",
    "#     iv_var='ndvi_v4',\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77ee50-7ea5-4b4a-bf6a-1ee61350b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "\n",
    "def multiple_regression(\n",
    "    data,\n",
    "    mediate_var='percent_home_norm',  # categorical\n",
    "    dv_var='control_norm',\n",
    "    iv_var='ndvi_v4',\n",
    "):\n",
    "    df = data.copy()\n",
    "\n",
    "    b_model = smf.ols(f\"{dv_var} ~ {iv_var} + C({mediate_var})\", data=df).fit()\n",
    "    print(b_model.summary())\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340bb186-752d-4c40-b592-ef7042ae3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### SEM ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb28435-ba1d-4cd5-91f2-804f3aa0e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mediation_sem(df, iv_var, mediate_var, dv_var):\n",
    "   \n",
    "    def p_to_stars(p):\n",
    "        if p < 0.001: return \"***\"\n",
    "        elif p < 0.01: return \"**\"\n",
    "        elif p < 0.05: return \"*\"\n",
    "        else: return \"\"\n",
    "\n",
    "    # Copy relevant data\n",
    "    df_model = df[[iv_var, mediate_var, dv_var]].copy()\n",
    "\n",
    "    df_model = df[[iv_var, mediate_var, dv_var]].dropna().copy()\n",
    "\n",
    "\n",
    "    # Z-score all variables\n",
    "    df_model['iv_z'] = zscore(df_model[iv_var])\n",
    "    df_model['med_z'] = zscore(df_model[mediate_var].astype(float))\n",
    "    df_model['dv_z'] = zscore(df_model[dv_var])\n",
    "\n",
    "    # Define SEM model syntax\n",
    "    model_desc = \"\"\"\n",
    "    med_z ~ iv_z\n",
    "    dv_z ~ med_z + iv_z\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit model\n",
    "    model = Model(model_desc)\n",
    "    model.fit(df_model)\n",
    "\n",
    "    # Extract parameter estimates\n",
    "    estimates_df = model.inspect()\n",
    "\n",
    "    # Extract values for paths\n",
    "    a_row = estimates_df[(estimates_df['lval'] == 'med_z') & (estimates_df['rval'] == 'iv_z')]\n",
    "    b_row = estimates_df[(estimates_df['lval'] == 'dv_z') & (estimates_df['rval'] == 'med_z')]\n",
    "    c_row = estimates_df[(estimates_df['lval'] == 'dv_z') & (estimates_df['rval'] == 'iv_z')]\n",
    "\n",
    "    # Coefficients\n",
    "    a = a_row['Estimate'].values[0]\n",
    "    b = b_row['Estimate'].values[0]\n",
    "    c_prime = c_row['Estimate'].values[0]\n",
    "\n",
    "    # p-values\n",
    "    a_p = a_row['p-value'].values[0]\n",
    "    b_p = b_row['p-value'].values[0]\n",
    "    c_p = c_row['p-value'].values[0]\n",
    "\n",
    "    # Effects\n",
    "    indirect = a * b\n",
    "    total = indirect + c_prime\n",
    "\n",
    "    # Summary table\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Effect': [\n",
    "            'a (IV → Mediator)',\n",
    "            'b (Mediator → DV)',\n",
    "            \"c′ (Direct IV → DV)\",\n",
    "            'Indirect (a*b)',\n",
    "            'Total (a*b + c′)'\n",
    "        ],\n",
    "        'Estimate': [\n",
    "            round(a, 3),\n",
    "            round(b, 3),\n",
    "            round(c_prime, 3),\n",
    "            round(indirect, 3),\n",
    "            round(total, 3)\n",
    "        ],\n",
    "        'p-value': [\n",
    "            round(a_p, 4),\n",
    "            round(b_p, 4),\n",
    "            round(c_p, 4),\n",
    "            '',  # p for indirect not available without bootstrapping\n",
    "            ''\n",
    "        ],\n",
    "        'Significance': [\n",
    "            p_to_stars(a_p),\n",
    "            p_to_stars(b_p),\n",
    "            p_to_stars(c_p),\n",
    "            '',\n",
    "            ''\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810ed9c-eb7a-4e02-8809-c36a9d90dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Data Analysis Samples ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec235e2-1e2b-48b1-9864-a0d89f13b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_mediation_sem(\n",
    "#     df=us_merged_daily_chr,\n",
    "#     iv_var='ndvi_v4',\n",
    "#     mediate_var='education',\n",
    "#     dv_var='bprs_total_m2_norm'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2ae3c3-5e28-4a34-b243-a1d68e7b4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Data Analysis Visualizations ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3c6d9-3233-4ebe-9ac8-c0f4f572cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Significan Mean Difference (CHR + HC) -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b57311-20e2-4490-850c-2ca3fdfe42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_comparison_multiple(df_list, scales, labels_list):\n",
    "    \"\"\"\n",
    "    Plot grouped bar graph with error bars and significance markers comparing CHR vs HC groups across multiple timepoints.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_list: list of pandas DataFrames for different timepoints (bl, m1, m2)\n",
    "    - scales: dict of scales {label: column_name}\n",
    "    - labels_list: list of str labels for each timepoint ([\"Baseline\", \"Month 1\", \"Month 2\"])\n",
    "    \n",
    "    Returns:\n",
    "    - results_dict: {label: t_test_df} for each timepoint\n",
    "    - sem_dict: {label: sem_df} for each timepoint\n",
    "    \"\"\"\n",
    "    num_timepoints = len(df_list)\n",
    "    labels = [label.upper() for label in scales.keys()]\n",
    "    x = np.arange(len(labels))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(22, 10))\n",
    "\n",
    "    bar_width = 0.2\n",
    "    offset = -bar_width * (num_timepoints - 1) / 2\n",
    "\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # blue, orange, green\n",
    "\n",
    "    results_dict = {}\n",
    "    sem_dict = {}\n",
    "\n",
    "    for idx, (df, timepoint_label) in enumerate(zip(df_list, labels_list)):\n",
    "        df = df.copy()\n",
    "        df['Phenotype_group'] = df['Phenotype'].apply(lambda x: 'CHR' if x == 'CHR' else 'HC')\n",
    "\n",
    "        t_test_results = []\n",
    "        chr_errors = []\n",
    "        hc_errors = []\n",
    "\n",
    "        for scale_label, col_name in scales.items():\n",
    "            chr_group = df[df['Phenotype_group'] == 'CHR'][col_name].dropna()\n",
    "            hc_group = df[df['Phenotype_group'] == 'HC'][col_name].dropna()\n",
    "\n",
    "            t_stat, p_val = ttest_ind(chr_group, hc_group, equal_var=False)\n",
    "\n",
    "            t_test_results.append({\n",
    "                'n_chr': len(chr_group),\n",
    "                'n_hc': len(hc_group),\n",
    "                'Scale': scale_label.upper(),\n",
    "                'CHR Mean': round(chr_group.mean(), 3),\n",
    "                'HC Mean': round(hc_group.mean(), 3),\n",
    "                't-statistic': round(t_stat, 3),\n",
    "                'p-value': round(p_val, 4),\n",
    "                'Significance': '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
    "            })\n",
    "\n",
    "            chr_errors.append(round(sem(chr_group), 3))\n",
    "            hc_errors.append(round(sem(hc_group), 3))\n",
    "\n",
    "        t_test_df = pd.DataFrame(t_test_results)\n",
    "        sem_df = pd.DataFrame({\n",
    "            'Scale': list(scales.keys()),\n",
    "            f'{timepoint_label} CHR SEM': chr_errors,\n",
    "            f'{timepoint_label} HC SEM': hc_errors\n",
    "        })\n",
    "\n",
    "        results_dict[timepoint_label] = t_test_df\n",
    "        sem_dict[timepoint_label] = sem_df\n",
    "\n",
    "        chr_means = t_test_df['CHR Mean']\n",
    "        hc_means = t_test_df['HC Mean']\n",
    "\n",
    "        ax.bar(x + offset + idx * bar_width, chr_means, bar_width, \n",
    "               yerr=chr_errors, capsize=4, label=f'CHR - {timepoint_label}', alpha=0.8, color=colors[idx])\n",
    "        ax.bar(x + offset + idx * bar_width, hc_means, bar_width, \n",
    "               yerr=hc_errors, capsize=4, label=f'HC - {timepoint_label}', alpha=0.3, color=colors[idx])\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Normalized Mean Score')\n",
    "    ax.set_title('Symptom Scores by Group (CHR vs HC) Across Timepoints')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return results_dict, sem_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632c712-0283-4c2d-83d7-59148f558c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Simpsons Paradox Plot ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8a70e-e059-4bd7-97b6-2104850f703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpsons_paradox_plot(df, x_col, y_col, group_col='Greenspace'):\n",
    "    # Drop NA\n",
    "    data = df[[x_col, y_col, group_col]].dropna()\n",
    "\n",
    "    # Use lmplot to plot group-specific regression\n",
    "    g = sns.lmplot(\n",
    "        data=data,\n",
    "        x=x_col,\n",
    "        y=y_col,\n",
    "        hue=group_col,\n",
    "        palette={'High': 'green', 'Low': 'gold'},\n",
    "        markers=['o', 's'],\n",
    "        height=10,\n",
    "        aspect=1.4,\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    ax = g.ax\n",
    "\n",
    "    # Plot overall regression line WITHOUT label first\n",
    "    overall_model = sm.OLS.from_formula(f\"{y_col} ~ {x_col}\", data=data).fit()\n",
    "    x_vals = np.linspace(data[x_col].min(), data[x_col].max(), 100)\n",
    "    y_vals = overall_model.params['Intercept'] + overall_model.params[x_col] * x_vals\n",
    "    overall_line, = ax.plot(x_vals, y_vals, color='black', linestyle='--')\n",
    "\n",
    "    # Get existing legend handles/labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    # Append just one 'Overall Trend' entry\n",
    "    handles.append(overall_line)\n",
    "    labels.append('Overall Trend')\n",
    "\n",
    "    ax.legend(handles, labels)\n",
    "\n",
    "    ax.set_title(f\"{y_col} vs {x_col} by Greenspace Group\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Collect regression slope results into a list\n",
    "    regression_summary = []\n",
    "\n",
    "    for group in data[group_col].unique():\n",
    "        sub_data = data[data[group_col] == group]\n",
    "        model = sm.OLS.from_formula(f\"{y_col} ~ {x_col}\", data=sub_data).fit()\n",
    "        regression_summary.append({\n",
    "            'Group': group,\n",
    "            'Beta': round(model.params[x_col], 4),\n",
    "            'P-value': round(model.pvalues[x_col], 4)\n",
    "        })\n",
    "\n",
    "    # Add overall regression\n",
    "    regression_summary.append({\n",
    "        'Group': 'Overall',\n",
    "        'Beta': round(overall_model.params[x_col], 4),\n",
    "        'P-value': round(overall_model.pvalues[x_col], 4)\n",
    "    })\n",
    "\n",
    "    regression_df = pd.DataFrame(regression_summary)\n",
    "    return regression_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb2bea-13d6-47e1-b1e2-21c11fc45666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Sample Data Analysis ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc68e4-01a8-4f10-82aa-0c7385d54d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpsons_paradox_plot(us_merged_daily_chr, x_col='ndvi_v4', y_col='bprs_total_baseline_norm')\n",
    "# simpsons_paradox_plot(us_merged_daily_chr, x_col='ndvi_v4', y_col='bprs_total_m2_norm')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
